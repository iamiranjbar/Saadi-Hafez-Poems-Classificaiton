{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saadi vs. Hafez the Shirazians (Naive Bayes)\n",
    "In this project, we want to create a classifier for poems. This classifier will train with some poems from two iranian poets, Saadi and Hafez. With this training, we create a model for our classification. New instances predicted with this model and assign to a poet. <br>\n",
    "## Main idea\n",
    "I choose __word repetition__ in each poet poems as feature. At first, I calculate this dictionary for training set. And with this values, I calculate different values for bayes rule to predict test set. \n",
    "Input of our model is __x (poem)__ and our goal is __c (Poet of that poem)__. <br>\n",
    "In the bayes rule, we have:<br>\n",
    "<h1><center>$ P(c|x) = {{P(x|c)P(c)} \\over {P(x)}}$</center></h1><br>\n",
    "We want to calculate $P(c|x)$ that is <strong>posterior probability</strong>. This value means probability of a poet(Saadi or Hafez) given a poem. <br>\n",
    "<strong>Class prior probability</strong> $P(c)$ is ratio of one poet poems count to all poems. It means probability of that poet before any evidence.<br>\n",
    "<h4><center>$P(c=\"Saadi\") = {\"Saadi\" poems\\over all poems}$</center></h4><br>\n",
    "<strong>Evidence</strong> is the poems and by our selected feature is words of that poem. <strong>x</strong> is the poems and it devides to $x_1x_2x_3...x_n$ that each $x_i$ is a word. The probability of evidence is predictor prior probability $P(x)$. This probability in denominator is same for Saadi and hafez given this evidence probability calculation; So we can ignore calculating this and compare numerator of bayes rule for this two poets. <br>\n",
    "<strong>Likelihood</strong> is probabilty of a poem given a poet. This means how much it's possible that poem is for saadi or hafez. This probability calculated by below formula: <br>\n",
    "<h4><center>$P(x|c) = P(x_1x_2x_3...x_n|c)$</center></h4><br>\n",
    "In the naive bayes, we make a simplifier assumption that features are conditionally independent given label. So we can write and calculate it like below: <br>\n",
    "<h4><center>$P(x_1x_2x_3...x_n|c) = P(x_1|c) \\times P(x_2|c) \\times ... \\times P(x_n|C)$</center></h4><br>\n",
    "Each $P(x_i|c)$ denotes how much possible that word $x_i$ comes in poems by poet <i>c</i>.<br>\n",
    "<h4><center>$P(x_i|c) = {P(x_i, c)\\over {P(c)}}$</center></h4><br>\n",
    "P(c) had been discussed above. $P(x_i, c)$ calculated like below:<br>\n",
    "<h5><center>$P(x_i,c) = {Number Of Word x_i Repetition In c Poems\\over Number Of All Words Of c Poems}$</center></h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "In the first step, we open __train_test.csv__ file and read poems data. Then we devide it to two parts. We choose 80% of this poems randomly as training set and other 20% Then we'll parse poems and split them by space and half space and count words in each poet poems. We count total number of poems by Saadi and Hafez. Our __feature__ in this problem is __words (and their repetition count)__. Our __goal__ is to find poet of the poem.\n",
    "\n",
    "\n",
    "### Open and reading file\n",
    "We read file with _pandas csv reader_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>چون می‌رود این کشتی سرگشته که آخر</td>\n",
       "      <td>hafez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>که همین بود حد امکانش</td>\n",
       "      <td>saadi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ارادتی بنما تا سعادتی ببری</td>\n",
       "      <td>hafez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>خدا را زین معما پرده بردار</td>\n",
       "      <td>hafez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>گویی که در برابر چشمم مصوری</td>\n",
       "      <td>saadi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                text  label\n",
       "0  چون می‌رود این کشتی سرگشته که آخر  hafez\n",
       "1              که همین بود حد امکانش  saadi\n",
       "2         ارادتی بنما تا سعادتی ببری  hafez\n",
       "3         خدا را زین معما پرده بردار  hafez\n",
       "4        گویی که در برابر چشمم مصوری  saadi"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_test_file_path = \"./Data/train_test.csv\"\n",
    "train_test_data = pd.read_csv(train_test_file_path)\n",
    "train_test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading stopWords\n",
    "_Stop Words_ are the most repeated words in each language. We have some of this words in persian literature. We prepare a set of stop words and read them in this part. We remove this words from our training set and hence from our model. With this work, we try to remove not very important feature that got important because they have a very high repeat in poems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['و',\n",
       " 'در',\n",
       " 'به',\n",
       " 'از',\n",
       " 'كه',\n",
       " 'مي',\n",
       " 'اين',\n",
       " 'است',\n",
       " 'را',\n",
       " 'با',\n",
       " 'هاي',\n",
       " 'براي',\n",
       " 'آن',\n",
       " 'يك',\n",
       " 'شود',\n",
       " 'شده',\n",
       " 'خود',\n",
       " 'ها',\n",
       " 'كرد',\n",
       " 'شد',\n",
       " 'اي',\n",
       " 'تا',\n",
       " 'كند',\n",
       " 'بر',\n",
       " 'بود',\n",
       " 'نيز',\n",
       " 'وي',\n",
       " 'هم',\n",
       " 'كنند',\n",
       " 'دارد',\n",
       " 'ما',\n",
       " 'كرده',\n",
       " 'يا',\n",
       " 'اما',\n",
       " 'بايد',\n",
       " 'دو',\n",
       " 'اند',\n",
       " 'هر',\n",
       " 'خواهد',\n",
       " 'او',\n",
       " 'نمي',\n",
       " 'بين',\n",
       " 'پيش',\n",
       " 'پس',\n",
       " 'اگر',\n",
       " 'همه',\n",
       " 'صورت',\n",
       " 'يكي',\n",
       " 'هستند',\n",
       " 'بي',\n",
       " 'من',\n",
       " 'دهد',\n",
       " 'هزار',\n",
       " 'نيست',\n",
       " 'استفاده',\n",
       " 'داد',\n",
       " 'داشته',\n",
       " 'راه',\n",
       " 'داشت',\n",
       " 'چه',\n",
       " 'همچنين',\n",
       " 'كردند',\n",
       " 'داده',\n",
       " 'بوده',\n",
       " 'دارند',\n",
       " 'همين',\n",
       " 'ميليون',\n",
       " 'سوي',\n",
       " 'شوند',\n",
       " 'بيشتر',\n",
       " 'بسيار',\n",
       " 'روي',\n",
       " 'گرفته',\n",
       " 'هايي',\n",
       " 'تواند',\n",
       " 'هيچ',\n",
       " 'چند',\n",
       " 'جديد',\n",
       " 'بيش',\n",
       " 'شدن',\n",
       " 'كردن',\n",
       " 'كنيم',\n",
       " 'نشان',\n",
       " 'حتي',\n",
       " 'اينكه',\n",
       " 'ولی',\n",
       " 'توسط',\n",
       " 'چنين',\n",
       " 'برخي',\n",
       " 'نه',\n",
       " 'ديروز',\n",
       " 'دوم',\n",
       " 'بعد',\n",
       " 'گيرد',\n",
       " 'شما',\n",
       " 'گفته',\n",
       " 'آنان',\n",
       " 'بار',\n",
       " 'طور',\n",
       " 'گرفت',\n",
       " 'دهند',\n",
       " 'طي',\n",
       " 'بودند',\n",
       " 'ميليارد',\n",
       " 'بدون',\n",
       " 'كل',\n",
       " 'تر  براساس',\n",
       " 'شدند',\n",
       " 'ترين',\n",
       " 'باشند',\n",
       " 'ندارد',\n",
       " 'چون',\n",
       " 'گويد',\n",
       " 'ديگري',\n",
       " 'همان',\n",
       " 'خواهند',\n",
       " 'قبل',\n",
       " 'آمده',\n",
       " 'اكنون',\n",
       " 'تحت',\n",
       " 'طريق',\n",
       " 'گيري',\n",
       " 'جاي',\n",
       " 'هنوز',\n",
       " 'چرا',\n",
       " 'البته',\n",
       " 'كنيد',\n",
       " 'سوم',\n",
       " 'كنم',\n",
       " 'بلكه',\n",
       " 'زير',\n",
       " 'توانند',\n",
       " 'ضمن',\n",
       " 'فقط',\n",
       " 'بودن',\n",
       " 'حق',\n",
       " 'آيد',\n",
       " 'وقتي',\n",
       " 'اش',\n",
       " 'يابد',\n",
       " 'نخستين',\n",
       " 'امسال',\n",
       " 'تاكنون',\n",
       " 'مانند',\n",
       " 'تازه',\n",
       " 'آورد',\n",
       " 'فكر',\n",
       " 'آنچه',\n",
       " 'نخست',\n",
       " 'نشده',\n",
       " 'شايد',\n",
       " 'چهار',\n",
       " 'جريان',\n",
       " 'پنج',\n",
       " 'ساخته',\n",
       " 'زيرا',\n",
       " 'نزديك',\n",
       " 'برداري',\n",
       " 'كسي',\n",
       " 'ريزي',\n",
       " 'رفت',\n",
       " 'گردد',\n",
       " 'مثل',\n",
       " 'آمد',\n",
       " 'ام',\n",
       " 'بهترين',\n",
       " 'دانست',\n",
       " 'كمتر',\n",
       " 'دادن',\n",
       " 'تمامي',\n",
       " 'جلوگيري',\n",
       " 'بيشتري',\n",
       " 'ايم',\n",
       " 'ناشي',\n",
       " 'چيزي',\n",
       " 'آنكه',\n",
       " 'بالا',\n",
       " 'بنابراين',\n",
       " 'ايشان',\n",
       " 'بعضي',\n",
       " 'دادند',\n",
       " 'داشتند',\n",
       " 'نخواهد',\n",
       " 'نبايد',\n",
       " 'غير',\n",
       " 'نبود',\n",
       " 'ديده',\n",
       " 'وگو',\n",
       " 'داريم',\n",
       " 'چگونه',\n",
       " 'بندي',\n",
       " 'فوق',\n",
       " 'ده',\n",
       " 'هستيم',\n",
       " 'ديگران',\n",
       " 'همچنان',\n",
       " 'سراسر',\n",
       " 'ندارند',\n",
       " 'گروهي',\n",
       " 'سعي',\n",
       " 'آنجا',\n",
       " 'يكديگر',\n",
       " 'كردم',\n",
       " 'بيست',\n",
       " 'بروز',\n",
       " 'سپس',\n",
       " 'رفته',\n",
       " 'آورده',\n",
       " 'نمايد',\n",
       " 'باشيم',\n",
       " 'گويند',\n",
       " 'زياد',\n",
       " 'خويش',\n",
       " 'همواره',\n",
       " 'گذاشته',\n",
       " 'شش  نداشته',\n",
       " 'شناسي',\n",
       " 'خواهيم',\n",
       " 'داشتن',\n",
       " 'نظير',\n",
       " 'همچون',\n",
       " 'نكرده',\n",
       " 'شان',\n",
       " 'سابق',\n",
       " 'دانند',\n",
       " 'جايي',\n",
       " 'بی',\n",
       " 'برایِ',\n",
       " 'مثلِ',\n",
       " 'بارة',\n",
       " 'اثرِ',\n",
       " 'تولِ',\n",
       " 'علّتِ',\n",
       " 'سمتِ',\n",
       " 'عنوانِ',\n",
       " 'قصدِ',\n",
       " 'روب',\n",
       " 'کی',\n",
       " 'که',\n",
       " 'چیست',\n",
       " 'هست',\n",
       " 'کجا',\n",
       " 'کجاست',\n",
       " 'کَی',\n",
       " 'چطور',\n",
       " 'کدام',\n",
       " 'آیا',\n",
       " 'مگر',\n",
       " 'چندین',\n",
       " 'یک',\n",
       " 'چیزی',\n",
       " 'دیگر',\n",
       " 'کسی',\n",
       " 'بعری',\n",
       " 'هیچ',\n",
       " 'چیز',\n",
       " 'جا',\n",
       " 'کس',\n",
       " 'هرگز',\n",
       " 'یا',\n",
       " 'تنها',\n",
       " 'بلکه',\n",
       " 'خیاه',\n",
       " 'بله',\n",
       " 'بلی',\n",
       " 'آره',\n",
       " 'آری',\n",
       " 'مرسی',\n",
       " 'البتّه',\n",
       " 'لطفاً',\n",
       " 'ّه',\n",
       " 'انکه',\n",
       " 'وقتیکه',\n",
       " 'همین',\n",
       " 'پیش',\n",
       " 'مدّتی',\n",
       " 'هنگامی',\n",
       " 'مان',\n",
       " 'تان',\n",
       " 'تو',\n",
       " 'ست',\n",
       " 'این',\n",
       " 'گر',\n",
       " 'ز',\n",
       " 'ای',\n",
       " 'نمی',\n",
       " 'چو',\n",
       " 'نیست',\n",
       " '']"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_path = \"./stopWords.csv\"\n",
    "stop_words = pd.read_csv(stop_words_path)[\"کلمات\"].values\n",
    "stop_words = list(stop_words)\n",
    "stop_words.append('')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Devide dataset to train set (80%) and test set (20%)\n",
    "We should devide our set to two parts. _Training set_ for learning model and _Test set_ to calculate error of our model. We choose this way to avoid overfitting model to training set and get very bad result in operation. <br>\n",
    "We do this division by make a random number betwwen 0 and 1 and create a mask for data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mask = np.random.rand(len(train_test_data)) < 0.8\n",
    "train_set, test_set = train_test_data[mask], train_test_data[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training set statistics for using in probability calculation of our model\n",
    "In this part, we explore our training set and collect required data for using in our naive bayes rule.<br>\n",
    "We calculate poem count of each poet that it's in calculating P(c) and each $P{x_i|c}$. We calculate word repetition in each poet poems. This is the main feature in this problem and it's used to calculate all values of bayes rule. __poem_count__ is count of poet poems. __word_repetition__ is count of each word repetiotion in each poet poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hafez': 6740, 'saadi': 9944}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import operator\n",
    "\n",
    "poem_count = {}\n",
    "word_repetition = {}\n",
    "\n",
    "def increase_poem_count(poet):\n",
    "    if poet not in poem_count:\n",
    "        poem_count[poet] = 1\n",
    "    else:\n",
    "        poem_count[poet] += 1\n",
    "\n",
    "def update_word_repetition(poem, poet):\n",
    "    if poet not in word_repetition:\n",
    "        word_repetition[poet] = {}\n",
    "    words = re.split(' |\\u200c', poem)\n",
    "    for word in words:\n",
    "        if word not in word_repetition[poet]:\n",
    "            word_repetition[poet][word] = 1\n",
    "        else:\n",
    "            word_repetition[poet][word] += 1\n",
    "\n",
    "def extract_data(row):\n",
    "    poet = row['label']\n",
    "    poem = row['text']\n",
    "    increase_poem_count(poet)\n",
    "    update_word_repetition(poem, poet)\n",
    "\n",
    "train_set.apply(extract_data, axis=1)\n",
    "print(poem_count)\n",
    "# print(word_repetition)\n",
    "# print(sorted(word_repetition[\"saadi\"].items(), key=operator.itemgetter(1)))\n",
    "# print(sorted(word_repetition[\"hafez\"].items(), key=operator.itemgetter(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement base calculations for naive bayes\n",
    "This part function are base functions for calculate bayes rule values.<br>\n",
    "__get_poet_probability__ function calculates P(c) that c is the poet. <br>\n",
    "__get_partial_likelihood__ function calculates $P{x_i|c}$ that $x_i$ is the word and x is the poet. <br>\n",
    "__calculate_poets_posterior_probabilty__ function calculates final comparable values for each poet. It's calculate P(c|x) some how because it doesn't calculate P(x) in denominator but it calculates probabilty measure that can compare between two poets. <br> \n",
    "It gets a poem and calculate the posterior probability for Hafez and Saadi. It returns this two values. <br>\n",
    "This function has another input <i>ignore_zeros</i>. If this argument is True, it's ignore words that make probability zero for each poet. If a word have zero partial probability, we ignore it. If different words make two poets probability zero, we calculate the posterior probability another time without this word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poet_probability(poet):\n",
    "    return poem_count[poet] / sum(poem_count.values())\n",
    "\n",
    "def get_partial_likelihood(word, poet):\n",
    "    poet_probabilty = get_poet_probability(poet)\n",
    "    poet_words_count = sum(word_repetition[poet].values())\n",
    "    if word in word_repetition[poet]:\n",
    "        word_repeat = word_repetition[poet][word]\n",
    "    else:\n",
    "        word_repeat = 0\n",
    "    word_by_poet_probability = word_repeat / poet_words_count\n",
    "    return word_by_poet_probability / poet_probabilty\n",
    "\n",
    "def calculate_poets_posterior_probabilty(poem, ignore_zeros = False):\n",
    "    saadi_prior_probability = get_poet_probability('saadi')\n",
    "    hafez_prior_probability = get_poet_probability('hafez')\n",
    "    hafez_likelihood = 1\n",
    "    saadi_likelihood = 1\n",
    "    words = re.split(' |\\u200c', poem)\n",
    "    for word in words:\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        saadi_partial_likelihood = get_partial_likelihood(word, 'saadi')\n",
    "        hafez_partial_likelihood = get_partial_likelihood(word, 'hafez')\n",
    "        if ignore_zeros and (saadi_partial_likelihood == 0 or hafez_partial_likelihood == 0):\n",
    "            continue\n",
    "        if saadi_partial_likelihood == 0 and hafez_partial_likelihood == 0:\n",
    "            continue\n",
    "        saadi_likelihood *= saadi_partial_likelihood\n",
    "        hafez_likelihood *= hafez_partial_likelihood\n",
    "    saadi_postrior_probability = saadi_likelihood * saadi_prior_probability\n",
    "    hafez_postrior_probability = hafez_likelihood * hafez_prior_probability\n",
    "    return saadi_postrior_probability, hafez_postrior_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement predict function\n",
    "This function get a poem and calculates posterior probability for each poet. By comparing poets, We return the poet name as it's class. <br>\n",
    "If they have equal probability, We choose class randomly with the chance of it's weight. (Weights are poet poems count.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_poet(poem):\n",
    "    saadi_probability, hafez_probability = calculate_poets_posterior_probabilty(poem)\n",
    "    if saadi_probability == 0 and hafez_probability == 0:\n",
    "        saadi_probability, hafez_probability = calculate_poets_posterior_probabilty(poem, ignore_zeros=True)\n",
    "    if saadi_probability > hafez_probability:\n",
    "        return \"saadi\"\n",
    "    elif saadi_probability < hafez_probability:\n",
    "        return \"hafez\"\n",
    "    else:\n",
    "        random_index = np.random.randint(0, len(train_set)-1)\n",
    "        if random_index < poem_count[\"saadi\"]:\n",
    "            return \"saadi\"\n",
    "        else:\n",
    "            return \"hafez\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions\n",
    "We have three function for three evaluation function:\n",
    "1. __calculate_recall__ function calculates below formula:\n",
    "<h3><center>$Recall = {CorrectDetectedHafezes\\over AllHafezes}$</center></h3><br>\n",
    "2. __calculate_precision__ function calculates below formula:\n",
    "<h3><center>$Precision = {CorrectDetectedHafezes\\over DetectedHafezes}$</center></h3><br>\n",
    "3. __calculate_accuracy__ function calculates below formula:\n",
    "<h3><center>$Accuracy = {CorrectDetected\\over Total}$</center></h3><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(predicted_labels, actual_labels):\n",
    "    correct_predicted_hafez = 0\n",
    "    all_hafez_poems = 0\n",
    "    for i in range(len(actual_labels)):\n",
    "        if actual_labels[i] == \"hafez\":\n",
    "            all_hafez_poems += 1\n",
    "            if predicted_labels[i] == \"hafez\":\n",
    "                correct_predicted_hafez += 1\n",
    "    return correct_predicted_hafez / all_hafez_poems\n",
    "\n",
    "def calculate_precision(predicted_labels, actual_labels):\n",
    "    correct_predicted_hafez = 0\n",
    "    all_predicted_hafez = 0\n",
    "    for i in range(len(actual_labels)):\n",
    "        if predicted_labels[i] == \"hafez\":\n",
    "            all_predicted_hafez += 1\n",
    "            if actual_labels[i] == \"hafez\":\n",
    "                correct_predicted_hafez += 1\n",
    "    return correct_predicted_hafez / all_predicted_hafez\n",
    "\n",
    "def calculate_accuracy(predicted_labels, actual_labels):\n",
    "    all_poems = len(actual_labels)\n",
    "    correct_predicted_poems = sum([actual_labels[i] == predicted_labels[i] for i in range(len(actual_labels))])\n",
    "    return correct_predicted_poems / all_poems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict test set\n",
    "In this part, we predict test set labels and then find three values for errors. The error values are: <br>\n",
    "1. Recall = 82.03\n",
    "2. Precision = 62.74\n",
    "3. Accuracy = 72.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8203309692671394\n",
      "0.6274864376130199\n",
      "0.7299497246827867\n"
     ]
    }
   ],
   "source": [
    "test_poems = test_set.drop(columns=[\"label\"])\n",
    "predicted_poets = test_poems[\"text\"].apply(predict_poet)\n",
    "actual_poets = test_set.drop(columns=[\"text\"])\n",
    "print(calculate_recall(predicted_poets.values, actual_poets[\"label\"].values))\n",
    "print(calculate_precision(predicted_poets.values, actual_poets[\"label\"].values))\n",
    "print(calculate_accuracy(predicted_poets.values, actual_poets[\"label\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace smoothing\n",
    "Assume one word comes in a new poem. This word appear in previopus Saadi's poems but it doesn't appear in Hafez's poems. When we get partial_likelihood for this word and hafez we get 0. When we product it to previous probability, final probability of hafez becoms zero. Therefor it can't be hafez's poem and other words aren't important however they have very strong evidence for being hafez's poem. So our model decide it's saadi's poem and it can be wrong. <br>\n",
    "The idea for resolve this problem is __Laplace smoothing__. In this situtation, we choose a small probability for the word with 0 repetition. This probability doesn't not remove other evidences effect and doesn't have large impact on choosing a word(decrease probability ver much!).\n",
    "I have two ideas:\n",
    "1. Count words with zero repetition in poet previous poems and sum numerator with one and denominator with number of zero-repeated words. Sum of probability in this idea is 1 and we relate a non zero probability to a word with zero repetition.\n",
    "2. Sum numerator with one and denominator with number of all word counts.<br>\n",
    "I implementd second idea in __get_partial_laplace_likelihood__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_count = sum(word_repetition[\"saadi\"].values()) + sum(word_repetition[\"hafez\"].values())\n",
    "\n",
    "def get_partial_laplace_likelihood(word, poet):\n",
    "    poet_probabilty = get_poet_probability(poet)\n",
    "    poet_words_count = sum(word_repetition[poet].values())\n",
    "    if word in word_repetition[poet]:\n",
    "        word_repeat = word_repetition[poet][word]\n",
    "    else:\n",
    "        word_repeat = 0\n",
    "    word_by_poet_probability = (word_repeat + 1) / (poem_count[poet] + all_words_count)\n",
    "    return word_by_poet_probability / poet_probabilty\n",
    "\n",
    "def calculate_poets_posterior_probabilty_laplace(poem):\n",
    "    saadi_prior_probability = get_poet_probability('saadi')\n",
    "    hafez_prior_probability = get_poet_probability('hafez')\n",
    "    hafez_likelihood = 1\n",
    "    saadi_likelihood = 1\n",
    "    words = re.split(' |\\u200c', poem)\n",
    "    for word in words:\n",
    "        if word not in word_repetition[\"saadi\"] and word not in word_repetition[\"hafez\"]:\n",
    "            continue\n",
    "        saadi_partial_likelihood = get_partial_laplace_likelihood(word, 'saadi')\n",
    "        hafez_partial_likelihood = get_partial_laplace_likelihood(word, 'hafez')\n",
    "        saadi_likelihood *= saadi_partial_likelihood\n",
    "        hafez_likelihood *= hafez_partial_likelihood\n",
    "    saadi_postrior_probability = saadi_likelihood * saadi_prior_probability\n",
    "    hafez_postrior_probability = hafez_likelihood * hafez_prior_probability\n",
    "    return saadi_postrior_probability, hafez_postrior_probability\n",
    "\n",
    "def predict_poet_laplace(poem):\n",
    "    saadi_probability, hafez_probability = calculate_poets_posterior_probabilty_laplace(poem)\n",
    "    if saadi_probability > hafez_probability:\n",
    "        return \"saadi\"\n",
    "    elif saadi_probability < hafez_probability:\n",
    "        return \"hafez\"\n",
    "    else:\n",
    "        random_index = np.random.randint(0, len(train_set)-1)\n",
    "        if random_index < poem_count[\"saadi\"]:\n",
    "            return \"saadi\"\n",
    "        else:\n",
    "            return \"hafez\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test laplace smoothing\n",
    "Predict test set labels and calculate error values has this results:\n",
    "1. Recall = 80.39\n",
    "2. Precision = 70.33\n",
    "3. Accuracy = 78.64\n",
    "We see that all evaluation functions have good values and satisfy our needs. We have fair recall and precision and good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8039332538736591\n",
      "0.7033368091762252\n",
      "0.7864447086801427\n"
     ]
    }
   ],
   "source": [
    "test_poems = test_set.drop(columns=[\"label\"])\n",
    "predicted_poets = test_poems[\"text\"].apply(predict_poet_laplace)\n",
    "actual_poets = test_set.drop(columns=[\"text\"])\n",
    "print(calculate_recall(predicted_poets.values, actual_poets[\"label\"].values))\n",
    "print(calculate_precision(predicted_poets.values, actual_poets[\"label\"].values))\n",
    "print(calculate_accuracy(predicted_poets.values, actual_poets[\"label\"].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "1. why machine learning model with just high precision is not enough?<br>\n",
    "This model is not good always. For this problem, Assume a model that predict hafez when it's has ver higher probability than saadi. (For example, 100x) This model is very conservative and just predict hafez in very confident situations. This model has a very high precision(almost 100%) but has a very low recall because it doesn't classify other poems of hafez corrdctly and classify all of them as saadi's poem.\n",
    "2. Why just good accuracy is not enough for good model?<br>\n",
    "Assume our model just predict one class. (It set same label for all instances in test set.) Assume we have test dataset that is biased and more than 75% of data belongs to that class. Our not good model with no calculation get more than 75% accuracy in this dataset but it gets very low value of presicion and recall of other class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict evaluate.csv\n",
    "Run model against evaluate.csv for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n",
      "hafez\n",
      "saadi\n",
      "saadi\n"
     ]
    }
   ],
   "source": [
    "evaluate_file_path = \"./Data/evaluate.csv\"\n",
    "evaluate_data = pd.read_csv(evaluate_file_path)\n",
    "evaluate_poems = evaluate_data.drop(columns=[\"id\"])\n",
    "predicted_poets = evaluate_poems[\"text\"].apply(predict_poet_laplace)\n",
    "for i in range(len(predicted_poets)):\n",
    "    print(predicted_poets[i])\n",
    "# output_file_path = \"./Data/output.csv\"\n",
    "# pd.write_csv(output_file_path) !Lack of net :(("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
